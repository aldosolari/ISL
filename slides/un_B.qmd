---
title: "The Bias-Variance Trade-off"
subtitle: "Introduction to Statistical Learning - PISE"
author: "[Aldo Solari]{.orange}"
institute: "_Ca' Foscari University of Venice_"
page-layout: full
execute:
  cache: false
filters: 
  - remove-pause.lua
format:
  revealjs:
    auto-stretch: true
    center: true
    html-math-method: katex
    transition: none
    output-file: un_B_slides.html
    slide-number: true
    callout-appearance: minimal
    code-line-numbers: true
    theme: [default, ../template.css] # alternative themes (subset): default, night, dark
    embed-resources: false
    echo: false
    fig-dpi: 200
    # incremental: true  # Remove comment if you like incremental bullet points
    logo: img/cf_logo.png
    footer: "[ISL](https://aldosolari.github.io/ISL)"
    highlight-style: github
  html:
    html-math-method: katex
    echo: false
    callout-appearance: minimal
    theme: [simplex, ../template.css]
    toc: true
    toc-title: Table of contents
    embed-resources: false
    code-line-numbers: true
    smooth-scroll: true
    code-fold: false
    code-summary: "Show the code"
    fig-dpi: 150
    highlight-style: github
editor: 
  markdown: 
    wrap: 72
editor_options: 
  chunk_output_type: console
---


```{r}
#| warning: false
#| echo: false
#| include: false
#| message: false
#| purl: false
```


This unit will cover the following [topics]{.orange}:

-   Simple linear regression
-   Polynomial regression
-   Bias-variance trade-off
-   Cross-validation

# Yesterday's and tomorrow's data


## The signal and the noise

-   Let us presume that [yesterday]{.orange} we observed $n = 30$ pairs
    of data $(x_i, y_i)$.

-   Data were generated according to $$
      Y_i = f(x_i) + \epsilon_i, \quad i=1,\dots,n,
      $$ with each $y_i$ being the realization of $Y_i$.

-   The $\epsilon_1,\dots,\epsilon_n$ are iid "[error]{.orange}" terms,
    such that $\mathbb{E}(\epsilon_i)=0$ and
    $\mathbb{V}\text{ar}(\epsilon_i)=\sigma^2 = 10^{-4}$.

-   Here $f(x)$ is a regression function ([signal]{.blue}) that we leave
    unspecified .

-   [Tomorrow]{.blue} we will get a new $x$. We wish to
    [predict]{.orange} $Y$.

## 

```{r}
#| fig-width: 5
#| fig-height: 4.5
#| fig-align: center
#| warning: false

rm(list = ls())
library(tidyverse)
library(ggplot2)
library(ggthemes)

# The dataset can be downloaded here: https://aldosolari.github.io/ISL/data/yesterday.txt
dataset <- read.table("../data/yesterday.txt", header = TRUE)

ftrue <- c(
  0.4342, 0.4780, 0.5072, 0.5258, 0.5369,
  0.5426, 0.5447, 0.5444, 0.5425, 0.5397,
  0.5364, 0.5329, 0.5294, 0.5260, 0.5229,
  0.5200, 0.5174, 0.5151, 0.5131, 0.5113,
  0.5097, 0.5083, 0.5071, 0.5061, 0.5052,
  0.5044, 0.5037, 0.5032, 0.5027, 0.5023
)

ggplot(data = dataset, aes(x = x, y = ftrue)) +
  geom_line(color="blue") +
  geom_point(aes(x=1.6206897,y=0.5032543),colour="#fc7d0b") +
  geom_segment(aes(x = 1.6206897, xend = 1.6206897, y=0.5032543, yend = 0.5260), col = "#fc7d0b", linetype = "dotted") +
  theme_light() +
  scale_color_tableau(palette = "Color Blind") +
  xlab("x") +
  ylab("y")

```

## Yesterday's data


```{r}
#| fig-width: 5
#| fig-height: 4.5
#| fig-align: center
#| warning: false

ggplot(data = dataset, aes(x = x, y = y.yesterday)) +
  geom_point() +
  theme_light() +
  scale_color_tableau(palette = "Color Blind") +
  xlab("x") +
  ylab("y")
```

<!-- ## Overfitting: mistaking noise for the signal -->


<!-- ```{r} -->
<!-- #| fig-width: 5 -->
<!-- #| fig-height: 4.5 -->
<!-- #| fig-align: center -->
<!-- #| warning: false -->

<!-- degree = 15 -->
<!-- fit <- lm(y.yesterday ~ poly(x, degree = degree, raw = FALSE), data = dataset) -->
<!-- # Fitted values -->
<!-- x_seq <- seq(from = min(dataset$x), to = max(dataset$x), length = 30000)  -->
<!-- y_hat <- predict(fit, newdata = data.frame(x = x_seq)) -->
<!-- data_pred <- data.frame(x = x_seq, y_hat = y_hat) -->

<!-- ggplot(data = dataset, aes(x = x, y = y.yesterday)) + -->
<!--   geom_line(data = dataset, aes(x = x, y = ftrue), col="blue") + -->
<!--   geom_line(data = data_pred, aes(x = x_seq, y = y_hat)) + -->
<!--   geom_point() + -->
<!--   theme_light() + -->
<!--   scale_color_tableau(palette = "Color Blind") + -->
<!--   xlab("x") + -->
<!--   ylab("y") -->
<!-- ``` -->


## Simple linear regression

::: incremental
-   The function $f(x)$ is unknown, therefore, it should be estimated.

-   A simple approach is using [simple linear regression]{.blue}: $$
    f(x; \beta) = \beta_0 + \beta_1 x,
    $$ namely $f(x)$ is [approximated]{.orange} with a straight line where 
    $\beta_0$ and $\beta_1$ are two unknown constants that represent
the [intercept]{.blue} and [slope]{.blue}, also known as [coefficients]{.orange} or
[parameters]{.orange}
    
-   Giving some [estimates]{.orange} $\hat{\beta}_0$ and $\hat{\beta}_1$ for the model coefficients, we predict future values using 
    $$\hat{y} =  \hat\beta_0 + \hat\beta_1 x$$
    where $\hat{y}$ indicates a [prediction]{.blue} of $Y$ on the basis of $X=x$. The [hat]{.orange} symbol denotes an estimated value.
:::


## Estimation of the parameters by least squares

::: incremental
- Let $\hat y_i = \hat\beta_0 + \hat\beta_1 x_i$ be the prediction for $Y$
based on the $i$th value of $X$. Then $e_i = y_i - \hat y_i$ represents the 
$i$th [residual]{.blue} 

- We define the [residual sum of squares]{.blue} ($\mathrm{RSS}$) as
$$\mathrm{RSS} = e_1^2+e_2^2+\ldots+e_n^2,$$
or equivalently as
$$\mathrm{RSS} = (y_1 - \hat\beta_0 - \hat\beta_1 x_1)^2+(y_2 - \hat\beta_0 - 
\hat\beta_1 x_2)^2+\ldots+ (y_n- \hat\beta_0 - \hat\beta_1 x_n)^2.$$

- The [least squares]{.blue} approach chooses $\hat{\beta}_0$ and $\hat{\beta}_1$ to minimize the \mathrm{RSS}. The minimizing values can be shown to be
$$
\begin{aligned}
\hat{\beta}_1 &= \frac{\sum_{i=1}^{n}(x_i - \bar x)(y_i - \bar y)}{\sum_{i=1}^{n}(x_i - \bar x)^2}\\
\hat{\beta}_0 &= \bar y - \beta_1 \bar x
\end{aligned}
$$
where $\bar y = \frac{1}{n}\sum_{i=1}^n y_i$ and $\bar x = \frac{1}{n}\sum_{i=1}^n x_i$ are the sample means.
:::

## Least squares fit : $\hat y = 0.520627 - 0.003 x$

```{r}
#| fig-width: 5
#| fig-height: 4.5
#| fig-align: center
#| warning: false

fit <- lm(y.yesterday ~ x, data = dataset)
# Fitted values
x_seq <- sort(dataset$x)
y_hat <- predict(fit, newdata = data.frame(x = x_seq))
data_pred <- data.frame(x = x_seq, y_hat = y_hat)

ggplot(data = dataset, aes(x = x, y = y.yesterday)) +
  geom_line(data = data_pred, aes(x = x_seq, y = y_hat)) +
  geom_point() +
  theme_light() +
  scale_color_tableau(palette = "Color Blind") +
  xlab("x") +
  ylab("y") + 
geom_segment(
  aes(xend = x, y = y_hat, yend = y.yesterday),
  color = "#fc7d0b",
  linewidth = 0.4
)
```

## Assessing the Overall Accuracy of the Model

::: incremental
- We compute the [mean squared error]{.blue} ($\mathrm{MSE}$)
$$\mathrm{MSE} = \frac{1}{n} \sum_{i=1}^{n}(y_i - \hat{y}_i)^2  = \frac{1}{n}\mathrm{RSS}$$

- [R-squared]{.blue} or fraction of variance explained is
$$R^2 = \frac{\mathrm{TSS} - \mathrm{RSS}}{\mathrm{TSS}} = 1 - \frac{\mathrm{RSS}}{\mathrm{TSS}}$$
where $\mathrm{TSS} = \sum_{i=1}^{n}(y_i - \bar y)^2$ is the [total sum of squares]{.blue}.
:::

## 

| Quantity | Value | 
|---------|----------|
| RSS      | 0.0171726     | 
| MSE     | 0.00057242     | 
| TSS      | 0.01731363     | 
| $R^2$      | 0.008146     |

## Polynomial regression

::: incremental
-   The function $f(x)$ is unknown, therefore, it should be estimated.

-   A simple approach is using [polynomial regression]{.orange}: $$
    f(x; \beta) = \beta_0 + \beta_1 x + \beta_2 x^2 + \cdots + \beta_d x^{d},
    $$ namely $f(x)$ is [approximated]{.orange} with a polynomial of
    degree $d$ 

-   This model is linear in the parameters: ordinary least squares can
    be applied.

-   How do we choose the [degree of the polynomial]{.blue} $d$?

-   Without clear guidance, in principle, any value of
    $d \in \{0,\dots,n-1\}$ could be appropriate.

-   Let us compare the [mean squared error]{.blue} (MSE) on yesterday's
    data ([training]{.orange}) $$
    \text{MSE}_{\text{train}} = \frac{1}{n}\sum_{i=1}^n\{y_i -f(x_i; \hat{\beta})\}^2,
    $$ or alternatively $R^2_\text{train}$, for different values of
    $d$...
:::


## Estimating Polynomial Regression

Let

$$
\mathbf{y} = (y_1,\dots,y_n)^\top
$$

be the vector of observed responses, and let $\mathbf{X} \in \mathbb{R}^{n \times (d+1)}$
be the design matrix whose $i$-th row is

$$
(1, x_i, x_i^2, \dots, x_i^d).
$$

[Ordinary least squares]{.orange} (OLS) estimation chooses the coefficient vector

$$
\hat{\beta}
=
\begin{pmatrix}
\hat{\beta}_0 \\
\hat{\beta}_1 \\
\vdots \\
\hat{\beta}_d
\end{pmatrix}
\in \mathbb{R}^{d+1}
$$

to **minimize the residual sum of squares (RSS)**:

$$
\hat{\beta}
=
\arg\min_{\beta \in \mathbb{R}^{d+1}}
\sum_{i=1}^n
\bigl(y_i - f(x_i; \beta)\bigr)^2.
$$
Thus, OLS finds the coefficients that make the fitted polynomial
as close as possible (in squared distance) to the observed data.


## Solution

The solution to this optimization problem is

$$
\hat{\beta}
=
(\mathbf{X}^\top \mathbf{X})^{-1}
\mathbf{X}^\top \mathbf{y},
$$


We avoid matrix algebra whenever possible, but in the OLS formula it is useful to understand matrix multiplication and inversion.

## Matrix Multiplication

Suppose

$$
\mathbf{A} \in \mathbb{R}^{r \times d},
\quad
\mathbf{B} \in \mathbb{R}^{d \times s}.
$$

Then the product $\mathbf{A}\mathbf{B}$ is an $r \times s$ matrix defined only if the number of columns of $\mathbf{A}$ equals the number of rows of $\mathbf{B}$.

The $(i,j)$-th element of $\mathbf{A}\mathbf{B}$ is

$$
(\mathbf{A}\mathbf{B})_{ij}
=
\sum_{k=1}^{d} a_{ik} b_{kj}.
$$

That is:

- Take the $i$-th row of $\mathbf{A}$,
- Take the $j$-th column of $\mathbf{B}$,
- Multiply corresponding elements,
- Sum the results.



## Matrix Inverse

A square matrix $\mathbf{A} \in \mathbb{R}^{p \times p}$ has an inverse
$\mathbf{A}^{-1}$ if

$$
\mathbf{A}\mathbf{A}^{-1}
=
\mathbf{A}^{-1}\mathbf{A}
=
\mathbf{I}_p,
$$

where $\mathbf{I}_p$ is the $p \times p$ identity matrix.

The inverse exists only if:

- $\mathbf{A}$ is square,
- $\mathbf{A}$ is nonsingular (its columns are linearly independent).


## Yesterday's data, polynomial regression

```{r}
#| fig-width: 7.8
#| fig-height: 4.55
#| fig-align: center

# Degrees of the polynomials
degree_list <- c(1, 3, 5, 11, 17, 23)

# I am using 30.000 obs to improve the quality of the graph
x_seq <- seq(from = min(dataset$x), to = max(dataset$x), length = 30000) 

# Actual fitting procedure
data_pred <- NULL
for (degree in degree_list) {
  # Fitting a polynomial of degree p - 1
  fit <- lm(y.yesterday ~ poly(x, degree = degree, raw = FALSE), data = dataset)
  # Fitted values
  y_hat <- predict(fit, newdata = data.frame(x = x_seq))
  data_pred <- rbind(data_pred, data.frame(x = x_seq, y_hat = y_hat, degree = paste("Degree d:", degree)))
}

# Graphical adjustment to get the plots in the right order
data_pred$degree <- factor(data_pred$degree)
data_pred$degree <- factor(data_pred$degree, levels = levels(data_pred$degree)[c(3, 5, 6, 1, 2, 4)])

# Final plot
ggplot(data = data_pred) +
  geom_line(aes(x = x, y = y_hat, col = degree)) +
  geom_point(data = dataset, aes(x = x, y = y.yesterday), size = 0.8) +
  theme_light() +
  theme(legend.position = "none") +
  facet_wrap(. ~ degree, ncol = 3) +
  scale_color_tableau(palette = "Color Blind") +
  xlab("x") +
  ylab("y") +
  ylim(c(0.42, 0.56)) # Manual identification of an "interesting" region
```

## Yesterday's data, goodness of fit

```{r}
# Main chunk of code; fitting several models and storing some relevant quantities
degree_list <- 1:23

# Tomorrow's mean-squared error
MSE_tot <- mean((dataset$y.tomorrow - mean(dataset$y.tomorrow))^2)

# Initialization
data_goodness <- data.frame(degree = degree_list, MSE = NA, R_squared = NA, MSE_test = NA, R_squared_test = NA)

# Code execution
for (degree in degree_list) {
  # Fitting a polynomial of degree p -1
  fit <- lm(y.yesterday ~ poly(x, degree = degree, raw = FALSE), data = dataset)
  y_hat <- fitted(fit)
  
  # Training goodness of fit
  data_goodness$MSE[degree] <- mean((dataset$y.yesterday - y_hat)^2)
  data_goodness$R_squared[degree] <- summary(fit)$r.squared
  
  # Test goodness of fit
  data_goodness$MSE_test[degree] <- mean((dataset$y.tomorrow - y_hat)^2)
  data_goodness$R_squared_test[degree] <- 1 - data_goodness$MSE_test[degree] / MSE_tot
}
```

::: columns
::: {.column width="50%"}
```{r}
#| fig-width: 5
#| fig-height: 4.5
ggplot(data = data_goodness, aes(x = degree, y = MSE)) +
  geom_line() +
  geom_point() +
  theme_light() +
  xlab("Degree d") +
  ylab("MSE")
```
:::

::: {.column width="50%"}
```{r}
#| fig-width: 5
#| fig-height: 4.5
ggplot(data = data_goodness, aes(x = degree, y = R_squared)) +
  geom_line() +
  geom_point() +
  theme_light() +
  xlab("Degree d") +
  ylab(expression(R^2))
```
:::
:::

## Yesterday's data, polynomial interpolation ($d = n-1$) {#yesterdays-data-polynomial-interpolation-p-n}

```{r}
#| fig-width: 9
#| fig-height: 6
#| fig-align: center
lagrange <- function(x0, y0) {
  f <- function(x) {
    sum(y0 * sapply(seq_along(x0), function(j) {prod(x - x0[-j]) / prod(x0[j] - x0[-j])}))
  }
  Vectorize(f, "x")
}
f <- lagrange(dataset$x, dataset$y.yesterday)

plot(dataset$x, dataset$y.yesterday, pch = 16, xlab = "x", ylab = "y", main = "Degree of the polynomial: n-1")
curve(f(x), n = 300, add = TRUE)
```

## Yesterday's data, tomorrow's prediction

::: incremental
-   The [MSE]{.blue} decreases as the number of parameter increases;
    similarly, the [$R^2$]{.blue} increases as a function of $d$. It can
    be [proved]{.orange} that this [always happens]{.orange} using
    ordinary least squares.

-   One might be tempted to let $d$ as large as possible to make the
    model more flexible...

-   Taking this reasoning to the extreme would lead to the choice
    $d = n-1$, so that $$
    \text{MSE}_\text{train} = 0, \qquad R^2_\text{train} = 1,
    $$ i.e., a perfect fit. This procedure is called
    [interpolation]{.blue}.

-   However, we are [not]{.orange} interested in predicting
    [yesterday]{.orange} data. Our goal is to predict
    [tomorrow]{.blue}'s data, i.e. a [new set]{.blue} of $n = 30$
    points: $$
    (x_1, \tilde{y}_1), \dots, (x_n, \tilde{y}_n), 
    $$ using $\hat{y}_i = f(x_i; \hat{\beta})$, where $\hat{\beta}$ is
    obtained using yesterday's data.

-   [Remark]{.orange}. Tomorrow's r.v. $\tilde{Y}_1,\dots, \tilde{Y}_n$
    follow the same scheme as yesterday's data.
:::

## Tomorrow's data, polynomial regression

```{r}
#| fig-width: 8
#| fig-height: 4.5
#| fig-align: center
ggplot(data = data_pred) +
  geom_line(aes(x = x, y = y_hat, col = degree)) +
  geom_point(data = dataset, aes(x = x, y = y.tomorrow), size = 0.8) +
  theme_light() +
  theme(legend.position = "none") +
  facet_wrap(. ~ degree, ncol = 3) +
  scale_color_tableau(palette = "Color Blind") +
  xlab("x") +
  ylab("y") +
  ylim(c(0.42, 0.56))
```

## Tomorrow's data, goodness of fit

::: columns
::: {.column width="50%"}
```{r}
#| fig-width: 5
#| fig-height: 4.5
ggplot(data = data_goodness, aes(x = degree, y = MSE_test)) +
  geom_line() +
  geom_point() +
  theme_light() +
  xlab("Degree d") +
  ylab("MSE")
```
:::

::: {.column width="50%"}
```{r}
#| fig-width: 5
#| fig-height: 4.5
ggplot(data = data_goodness, aes(x = degree, y = R_squared_test)) +
  geom_line() +
  geom_point() +
  theme_light() +
  xlab("Degree d") +
  ylab(expression(R^2))
```
:::
:::

## MSE on training and test set

```{r}
#| fig-width: 8
#| fig-height: 4.5
#| fig-align: center

# Number of degrees of the polynomial
degree_list <- 1:23
# Number of parameters in the model
p_list <- degree_list + 1

data_bv <- data.frame(p = p_list, #MSE = sigmatrue^2 + Bias2s + Vars, 
                      MSE_train = data_goodness$MSE, MSE_test = data_goodness$MSE_test)
data_bv <- reshape2::melt(data_bv, id = "p")
levels(data_bv$variable) <- c("MSE train (yesterday's data)", "MSE test (tomorrow's data)")
colnames(data_bv) <- c("p", "Error term", "value")

ggplot(data = data_bv, aes(x = p, y = value, col = `Error term`)) +
  geom_line() +
  geom_point() +
  geom_vline(xintercept = 5, linetype = "dotted") +
  theme_light() +
  theme(legend.position = "top") +
  scale_color_tableau(palette = "Color Blind") +
  xlab("Model complexity (p)") +
  ylab("Error")
```



## Comments and remarks

::: incremental
-   The mean squared error on tomorrow's data ([test]{.blue}) is defined
    as $$
    \text{MSE}_{\text{test}} = \frac{1}{n}\sum_{i=1}^n\{\tilde{y}_i -f(x_i; \hat{\beta})\}^2,
    $$ and similarly the $R^2_\text{test}$. We would like the
    $\text{MSE}_{\text{test}}$ to be [as small as possible]{.orange}.

-   For [small values]{.blue} of $d$, an increase in the degree of the
    polynomial [improves the fit]{.blue}. In other words, at the
    beginning, both the $\text{MSE}_{\text{train}}$ and the
    $\text{MSE}_{\text{test}}$ decrease.

-   For [larger values]{.orange} of $d$, the improvement gradually
    ceases, and the polynomial follows [random fluctuations]{.orange} in
    yesterday's data, which are [not observed]{.blue} in the [new
    sample]{.blue}.

-   An over-adaptation to yesterday's data is called
    [overfitting]{.orange}, which occurs when the training
    $\text{MSE}_{\text{train}}$ is low but the test
    $\text{MSE}_{\text{test}}$ is high.

-   Yesterday's dataset is available from the website of the textbook Azzalini and Scarpa. (2013):

    -   Dataset <http://azzalini.stat.unipd.it/Book-DM/yesterday.dat>
    -   True $f(\bm{x})$
        <http://azzalini.stat.unipd.it/Book-DM/f_true.R>
:::






# The Bias-Variance Trade-Off


## Prediction error

Suppose we predict $\tilde Y_i$ by $\hat{f}(x_i)$ where $\hat f$ is estimated from the training data. 

Our goal is to minimize the (expected) prediction error
$$\mathbb{E}[\text{MSE}_{\text{test}}] = \frac{1}{n}\sum_{i=1}^n\mathbb{E}\Big[ (\tilde{Y}_i -\hat f(x_i) )^2 \Big],$$ 
where the response variable is given by a signal-plus-noise model, 
$$Y = f(x) + \varepsilon,$$
with $f(x)$ representing the true underlying signal and $\varepsilon$ the random error term

## Sources of Error

- **Irreducible Error**
Can we make predictions without committing errors?
No — not even if we knew the true function $f$, because of the presence of the error term $\varepsilon$

- **Bias**
How far (on average) is the estimator $\hat f$ from the true function $f$?
For example, if we estimate a linear regression line when the true relationship is quadratic.

- **Variance**
How variable is the estimator $\hat f$?
In other words, how much would our estimates change if we computed them using different training sets?

## Reducible and Irreducible Error


$$
\mathbb{E}\big[(\tilde Y_i - \hat{f}(x_i))^2\big]
=
\mathbb{E}\big[(f(x_i) + \tilde \varepsilon_i - \hat{f}(x_i))^2\big].
$$

Expanding and using $\mathbb{E}[\tilde \varepsilon_i] = 0$,

$$
=
\underbrace{\mathbb{E}\big[(f(x_i) - \hat{f}(x_i))^2\big]}_{\text{Reducible error}}
+
\underbrace{\mathrm{Var}(\tilde \varepsilon_i)}_{\text{Irreducible error}}.
$$

The [prediction error]{.orange} is the sum of a [reducible]{.blue} and an [irreducible]{.blue} component.

## Reducible error: the bias-variance tradeoff

The reducible error can be further decomposed into the [squared bias]{.blue} and the [variance]{.blue} of the estimator $\hat{f}$.

$$
\begin{aligned}
\mathbb{E}\big[(f(x_i) - \hat{f}(x_i))^2\big]
&= \mathbb{E}\big[(f(x_i) - \mathbb{E}\hat{f}(x_i) + \mathbb{E}\hat{f}(x_i) - \hat{f}(x_i))^2\big] \\
&= \underbrace{\big(f(x_i) - \mathbb{E}\hat{f}(x_i)\big)^2}_{\text{Bias}^2}
+ \underbrace{\mathrm{Var}\big(\hat{f}(x_i)\big)}_{\text{Variance}}.
\end{aligned}
$$

Bias and variance are competing quantities: reducing one typically increases the other.  
Hence, we must choose a [trade-off]{.orange} between bias and variance.

Typically as the [flexibility]{.blue} of $\hat f$ increases, its variance increases, and its bias decreases. 

## Low flexibility: high bias, low variance

```{r}
#| fig-align: center
#| warning: false

rm(list=ls())

library(ggplot2)
library(ggpubr)
library(reshape2)

sigmatrue <- 0.01
x <- seq(.5, 3, length = 30)
n <- length(x)

ftrue <- c(0.4342,0.4780,0.5072,0.5258,0.5369,
           0.5426,0.5447,0.5444,0.5425,0.5397,
           0.5364,0.5329,0.5294,0.5260,0.5229,
           0.5200,0.5174,0.5151,0.5131,0.5113,
           0.5097,0.5083,0.5071,0.5061,0.5052,
           0.5044,0.5037,0.5032,0.5027,0.5023)

# ---- Degree 3 simulation ----
B <- 100
d <- 3
set.seed(123)

sim <- function(d){
  y <- ftrue + rnorm(n, 0, sigmatrue)
  fit <- lm(y ~ poly(x, degree = d))
  fitted(fit)
}

yhats <- replicate(B, sim(d))
Ehatf <- rowMeans(yhats)

df_sim <- data.frame(
  x = rep(x, B),
  yhat = as.vector(yhats),
  Simulation = factor(rep(1:B, each = n))
)

df_true <- data.frame(x = x, y = ftrue)
df_mean <- data.frame(x = x, y = Ehatf)

# ---- Left plot ----
p_left <- ggplot() +
  geom_line(data = df_sim,
            aes(x = x, y = yhat, group = Simulation, color = "estimated f"),
            linewidth = 0.3, alpha = 0.6) +
  geom_line(data = df_true,
            aes(x = x, y = y, color = "true f"),
            linewidth = 1) +
  geom_line(data = df_mean,
            aes(x = x, y = y, color = "average estimated f"),
            linewidth = 1) +
  scale_color_manual(values = c("estimated f" = "gray70",
                                "true f" = "blue",
                                "average estimated f" = "black")) +
  scale_x_continuous(breaks = c(1,2,3)) +
  coord_cartesian(ylim = c(.45, .55)) +
  labs(title = "Polynomial regression (degree 3)",
       x = "x",
       y = "Fitted values",
       color = NULL) +
  theme_pubr(base_size = 10) +
  theme(legend.position = "bottom",
        legend.direction = "horizontal",
        legend.text = element_text(size = 8))

# ---- Bias² and Variance ----
X <- model.matrix(lm(ftrue ~ poly(x, degree = d)))
invXtX <- solve(crossprod(X))

Bias2 <- (apply(X, 1, function(xrow)
  xrow %*% invXtX %*% t(X) %*% ftrue) - ftrue)^2

Var <- apply(X, 1, function(xrow)
  sigmatrue^2 * t(xrow) %*% invXtX %*% xrow
)

df_bv <- data.frame(
  x = x,
  Bias2 = as.numeric(Bias2),
  Var = as.numeric(Var)
)

df_bv_long <- melt(df_bv,
                   id.vars = "x",
                   variable.name = "Component",
                   value.name = "Value")

# ---- Right plot ----
p_right <- ggplot(df_bv_long, aes(x = x, y = Value, fill = Component)) +
  geom_bar(stat = "identity", width = 0.07) +
  scale_x_continuous(breaks = c(1,2,3)) +
  labs(title = "Bias² and Variance (degree 3)",
       x = "x",
       y = "Bias² + Var",
       fill = NULL) +
  theme_pubr(base_size = 10) +
  theme(legend.position = "bottom",
        legend.direction = "horizontal",
        legend.text = element_text(size = 8))

# ---- Combine ----
fig <- ggarrange(p_left, p_right, ncol = 2, widths = c(1.2, 1))

fig
```


## High flexibility: low bias, high variance

```{r}
#| fig-align: center
#| warning: false


# ---- Degree 12 simulation ----
B <- 100
d <- 12
set.seed(123)

sim <- function(d){
  y <- ftrue + rnorm(n, 0, sigmatrue)
  fit <- lm(y ~ poly(x, degree = d))
  fitted(fit)
}

yhats <- replicate(B, sim(d))
Ehatf <- rowMeans(yhats)

df_sim <- data.frame(
  x = rep(x, B),
  yhat = as.vector(yhats),
  Simulation = factor(rep(1:B, each = n))
)

df_true <- data.frame(x = x, y = ftrue)
df_mean <- data.frame(x = x, y = Ehatf)

# ---- Left plot ----
p_left <- ggplot() +
  geom_line(data = df_sim,
            aes(x = x, y = yhat, group = Simulation, color = "estimated f"),
            linewidth = 0.3, alpha = 0.6) +
  geom_line(data = df_true,
            aes(x = x, y = y, color = "true f"),
            linewidth = 1) +
  geom_line(data = df_mean,
            aes(x = x, y = y, color = "average estimated f"),
            linewidth = 1) +
  scale_color_manual(values = c("estimated f" = "gray70",
                                "true f" = "blue",
                                "average estimated f" = "black")) +
  scale_x_continuous(breaks = c(1,2,3)) +
  coord_cartesian(ylim = c(.45, .55)) +
  labs(title = "Polynomial regression (degree 12)",
       x = "x",
       y = "Fitted values",
       color = NULL) +
  theme_pubr(base_size = 10) +
  theme(legend.position = "bottom",
        legend.direction = "horizontal",
        legend.text = element_text(size = 8))

# ---- Bias² and Variance ----
X <- model.matrix(lm(ftrue ~ poly(x, degree = d)))
invXtX <- solve(crossprod(X))

Bias2 <- (apply(X, 1, function(xrow)
  xrow %*% invXtX %*% t(X) %*% ftrue) - ftrue)^2

Var <- apply(X, 1, function(xrow)
  sigmatrue^2 * t(xrow) %*% invXtX %*% xrow
)

df_bv <- data.frame(
  x = x,
  Bias2 = as.numeric(Bias2),
  Var = as.numeric(Var)
)

df_bv_long <- melt(df_bv,
                   id.vars = "x",
                   variable.name = "Component",
                   value.name = "Value")

# ---- Right plot ----
p_right <- ggplot(df_bv_long, aes(x = x, y = Value, fill = Component)) +
  geom_bar(stat = "identity", width = 0.07) +
  scale_x_continuous(breaks = c(1,2,3)) +
  labs(title = "Bias² and Variance (degree 12)",
       x = "x",
       y = "Bias² + Var",
       fill = NULL) +
  theme_pubr(base_size = 10) +
  theme(legend.position = "bottom",
        legend.direction = "horizontal",
        legend.text = element_text(size = 8))

# ---- Combine ----
fig <- ggarrange(p_left, p_right, ncol = 2, widths = c(1.2, 1))

fig
```


## Optimal choice

```{r}
#| fig-align: center
#| warning: false


# ---- Degree 12 simulation ----
B <- 100
d <- 5
set.seed(123)

sim <- function(d){
  y <- ftrue + rnorm(n, 0, sigmatrue)
  fit <- lm(y ~ poly(x, degree = d))
  fitted(fit)
}

yhats <- replicate(B, sim(d))
Ehatf <- rowMeans(yhats)

df_sim <- data.frame(
  x = rep(x, B),
  yhat = as.vector(yhats),
  Simulation = factor(rep(1:B, each = n))
)

df_true <- data.frame(x = x, y = ftrue)
df_mean <- data.frame(x = x, y = Ehatf)

# ---- Left plot ----
p_left <- ggplot() +
  geom_line(data = df_sim,
            aes(x = x, y = yhat, group = Simulation, color = "estimated f"),
            linewidth = 0.3, alpha = 0.6) +
  geom_line(data = df_true,
            aes(x = x, y = y, color = "true f"),
            linewidth = 1) +
  geom_line(data = df_mean,
            aes(x = x, y = y, color = "average estimated f"),
            linewidth = 1) +
  scale_color_manual(values = c("estimated f" = "gray70",
                                "true f" = "blue",
                                "average estimated f" = "black")) +
  scale_x_continuous(breaks = c(1,2,3)) +
  coord_cartesian(ylim = c(.45, .55)) +
  labs(title = "Polynomial regression (degree 5)",
       x = "x",
       y = "Fitted values",
       color = NULL) +
  theme_pubr(base_size = 10) +
  theme(legend.position = "bottom",
        legend.direction = "horizontal",
        legend.text = element_text(size = 8))

# ---- Bias² and Variance ----
X <- model.matrix(lm(ftrue ~ poly(x, degree = d)))
invXtX <- solve(crossprod(X))

Bias2 <- (apply(X, 1, function(xrow)
  xrow %*% invXtX %*% t(X) %*% ftrue) - ftrue)^2

Var <- apply(X, 1, function(xrow)
  sigmatrue^2 * t(xrow) %*% invXtX %*% xrow
)

df_bv <- data.frame(
  x = x,
  Bias2 = as.numeric(Bias2),
  Var = as.numeric(Var)
)

df_bv_long <- melt(df_bv,
                   id.vars = "x",
                   variable.name = "Component",
                   value.name = "Value")

# ---- Right plot ----
p_right <- ggplot(df_bv_long, aes(x = x, y = Value, fill = Component)) +
  geom_bar(stat = "identity", width = 0.07) +
  scale_x_continuous(breaks = c(1,2,3)) +
  labs(title = "Bias² and Variance (degree 5)",
       x = "x",
       y = "Bias² + Var",
       fill = NULL) +
  theme_pubr(base_size = 10) +
  theme(legend.position = "bottom",
        legend.direction = "horizontal",
        legend.text = element_text(size = 8))

# ---- Combine ----
fig <- ggarrange(p_left, p_right, ncol = 2, widths = c(1.2, 1))

fig
```




## Bias-Variance as a function of flexibility

```{r}
#| fig-align: center
#| warning: false

ds <- 1:20
ps <- ds + 1

Bias2s <- sapply(ps, function(p) 
  mean((ftrue - fitted(lm(ftrue ~ poly(x, degree = (p - 1)))))^2)
)

Vars <- ps * (sigmatrue^2) / n
Reds <- Bias2s + Vars

df_red <- data.frame(
  Degree = ds,
  `total Bias2` = Bias2s,
  `total Var` = Vars
)

df_long <- melt(df_red,
                id.vars = "Degree",
                variable.name = "Component",
                value.name = "Value")

p <- ggplot(df_long, aes(x = Degree, y = Value, fill = Component)) +
  geom_bar(stat = "identity", width = 0.7) +
  scale_x_continuous(breaks = seq(1, 20, by = 2)) +
  labs(
       x = "Flexibility (degree d)",
       y = "Reducible error",
       fill = NULL) +
  theme_pubr(base_size = 11) +
  theme(legend.position = "bottom",
        legend.direction = "horizontal")

p
```

## Comments and remarks

- **Reducible error** = [Bias²]{.orange} + [Variance]{.blue}

- Models with [low bias]{.orange} tend to have [high variance]{.blue}.

- Models with [low variance]{.blue} tend to have [high bias]{.orange}.

- On one hand, even if our model is **unbiased**,  
  the prediction error can still be large if the model is highly variable.

- On the other hand, a model that predicts a **constant** 
  has [zero variance]{.blue} but [high bias]{.orange}.

- To achieve good prediction performance, we must **balance** [bias]{.orange} and [variance]{.blue}.



# Cross-validation

## Training Error versus Test error

Recall the distinction between the [test error]{.orange} and the
[training error]{.blue}:

-  The [test error]{.orange} is the average error that results from using a
statistical learning method to predict the response on a new
observation, one that was not used in training the method.

-  In contrast, the [training error]{.blue} can be easily calculated by
applying the statistical learning method to the observations
used in its training.

-  But the [training error]{.blue} rate often is quite different from the
[test error]{.orange} rate, and in particular the former can
dramatically **underestimate** the latter.

## Training- versus Test-Set Performance

![](img/ELS_2_11.png){width="100%"}

Figure from the book [The Elements of Statistical Learning: Data Mining, Inference, and Prediction](https://hastie.su.domains/ElemStatLearn/). Second Edition February 2009. Trevor Hastie, Robert Tibshirani, Jerome Friedman.

## Validation-set approach

- Estimate
the test error by [holding out]{.orange} a subset of the training observations from the fitting process

- Here we randomly divide the available set of samples into
two parts: a [training set]]{.blue} and a [validation set]{.orange}.

- The model is fit on the training set, and the fitted model is
used to predict the responses for the observations in the
validation set.

- The resulting validation-set error provides an estimate of
the test error.

## The Validation process


![](img/5_1.png){width="100%"}

A random splitting into two halves: left part is training set,
right part is validation set

## Example: yesterday-tomorrow data 

- We randomly split the 30 observations into two sets: a [training set]]{.blue} with 15 data points and a [validation set]]{.orange} with the remaining 15 observations.

- We fit the polynomial regression model on the training set and compute the **MSE** on the validation set.

```{r}
#| fig-align: center
#| warning: false

library(readr)
library(boot)
library(ggplot2)
library(ggpubr)

PATH <- "http://azzalini.stat.unipd.it/Book-DM/yesterday.dat"
df <- read_table(PATH)

train <- data.frame(x = df$x, y = df$y.yesterday)
test  <- data.frame(x = df$x, y = df$y.tomorrow)

ds <- 1:12

# --- helper: 2-fold CV estimate for a given degree d ---
cv2_mse <- function(d){
  fit <- glm(y ~ poly(x, degree = d), data = train, family = gaussian)
  cv.glm(train, fit, K = 2)$delta[1]
}

# =========================
# Left panel: single split
# =========================
set.seed(123)
KCV1 <- sapply(ds, cv2_mse)

df_left <- data.frame(degree = ds, mse = KCV1)

p_left <- ggplot(df_left, aes(x = degree, y = mse)) +
  geom_line() +
  geom_point() +
  scale_x_continuous(breaks = ds) +   # integers only
  scale_y_log10() +
  labs(title = "Single split",
       x = "Degree (polynomial)",
       y = "Validation MSE") +
  theme_pubr(base_size = 11)

# ==========================
# Right panel: many splits
# ==========================
B <- 10
set.seed(123)

KCVmat <- replicate(B, sapply(ds, cv2_mse))

df_right <- data.frame(
  degree = rep(ds, times = B),
  mse = as.vector(KCVmat),
  split = factor(rep(1:B, each = length(ds)))
)

# minimum for each split
mins <- aggregate(mse ~ split, data = df_right, FUN = min)
df_min <- merge(df_right, mins, by = c("split", "mse"))

p_right <- ggplot(df_right,
                  aes(x = degree, y = mse, group = split, color = split)) +
  geom_line(alpha = 0.8) +
  geom_point(data = df_min, size = 2, show.legend = FALSE) +
  scale_x_continuous(breaks = ds) +   # integers only
  scale_y_log10() +
  labs(title = "Multiple splits",
       x = "Degree (polynomial)",
       y = "Validation MSE",
       color = "Split") +
  theme_pubr(base_size = 11)

# --- combine ---
fig <- ggarrange(p_left, p_right, ncol = 2, widths = c(1, 1))
fig
```


## Drawbacks of validation set approach

- The validation estimate of the test error can be [highly
variable]{.orange}, depending on precisely which observations are
included in the training set and which observations are
included in the validation set.

- In the validation approach, only a subset of the
observations — those that are included in the training set
rather than in the validation set — are used to fit the
model. This suggests that the validation set error may tend to
[overestimate]{.blue} the test error for the model fit on the entire data set.

## K-fold Cross-validation

-  Widely used approach for estimating test error.

-  Estimates can be used to select best model, and to give an
idea of the test error of the final chosen model.

-  Idea is to randomly divide the data into $K$ equal-sized
parts. We leave out part $k$, fit the model to the other
$K−1$ parts (combined), and then obtain predictions for
the left-out $k$th part.

- This is done in turn for each part $k= 1,2,...K$, and then
the results are combined.


## $K$-fold Cross-validation in detail

Divide data into $K$ roughly equal-sized parts ($K = 5$ here)

![](img/5_5.png){width="100%"}

## The details


- Let the $K$ parts (folds) be $C_1, C_2, \dots, C_K$,  
where $C_k$ denotes the indices of the observations in fold $k$.

- Let $n_k$ be the number of observations in fold $k$. Without loss of geenrality, suppose $n_k = n/K$.

- For each fold $k$, fit the model using all data **excluding** $C_k$,   and compute the validation mean squared error
$$
\text{MSE}_k
=
\frac{1}{n_k}
\sum_{i \in C_k}
\bigl(y_i - \hat{y}_i^{-k}\bigr)^2,
$$
where $\hat{y}_i^{-k}$ is the prediction for observation $i$ obtained from the model trained without fold $k$.

- The $K$-fold cross-validation estimate is their average: 
$$
\text{CV}(K)
=
\frac{1}{K}\sum_{k=1}^{K}
\text{MSE}_k.
$$
- Common choices are $K=5$ or $K=10$ It is quite evident that a larger $K$ requires more computations.

## Leave-One-Out Cross-Validation

- The maximum possible value for $K$ is $n$, yielding leave-one-out cross-validation (LOOCV).


![](img/5_3.png){width="100%"}


## Example: yesterday-tomorrow data 

- The LOOCV is hard to implement because it requires the estimation of $n$ different models.

- However, in ordinary least squares there is a brilliant computational shortcut.

```{r}
#| fig-align: center
#| warning: false

library(ggplot2)
library(ggpubr)
library(boot)

ds <- 1:12

# LOOCV via cv.glm
LOOCV <- sapply(ds, function(d)
  cv.glm(train,
         glm(y ~ poly(x, degree = d),
             data = train,
             family = gaussian))$delta[1]
)

df_loocv <- data.frame(
  degree = ds,
  LOOCV = LOOCV
)

# identify minimum
min_idx <- which.min(df_loocv$LOOCV)
df_min <- df_loocv[min_idx, ]

p <- ggplot(df_loocv, aes(x = degree, y = LOOCV)) +
  geom_line() +
  geom_point() +
  geom_point(data = df_min, color = "red", size = 3) +
  scale_x_continuous(breaks = ds) +
  labs(x = "Degree (polynomial)",
       y = "LOOCV MSE") +
  theme_pubr(base_size = 11)

p
```

## On the Choice of $K$


* A $K$-fold cross-validation with $K = 5$ or $K = 10$ provides an **upward biased** estimate of the true prediction error $\mathrm{Err}$, because each model is trained on fewer observations than the full dataset (either $4/5$ or $9/10$ of the data).

* Leave-one-out cross-validation (LOOCV) has **very small bias**, since each model is trained on $n - 1$ observations.  
However, it has **high variance**, because it averages $n$ highly positively correlated error estimates.

* Overall, the choice of $K$ is largely **context-dependent**, balancing bias, variance, and computational cost.


## Required readings from the textbook and course materials


- **Chapter 2: Statistical Learning**
  - 2.2 Assessing Model Accuracy
  - 2.2.1 Measuring the Quality of Fit
  - 2.2.2 The Bias-Variance Trade-Off
  
- **Chapter 5: Resampling Methods**
  - 5.1 Cross-Validation
    - 5.1.1 The Validation Set Approach
    - 5.1.2 Leave-One-Out Cross-Validation
    - 5.1.3 k-Fold Cross-Validation
   
- **Chapter 7: Moving Beyond Linearity**
   - 7.1 Polynomial Regression

[Video SL 2.3 Model Selection and Bias Variance Tradeoff - 10:05](https://www.youtube.com/playlist?list=PLoROMvodv4rOzrYsAxzQyHb8n_RWNuS1e)

[Video SL 5.1 Cross Validation - 14:02 ](https://www.youtube.com/playlist?list=PLoROMvodv4rOzrYsAxzQyHb8n_RWNuS1e)

[Video SL 5.2 K-fold Cross Validation - 13:34 ](https://www.youtube.com/playlist?list=PLoROMvodv4rOzrYsAxzQyHb8n_RWNuS1e)

[Video SL 7.1 Polynomials and Step Functions - first 7 minutes](https://www.youtube.com/playlist?list=PLoROMvodv4rOzrYsAxzQyHb8n_RWNuS1e)


